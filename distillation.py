import os
import json
import subprocess
import torch
import shutil
from datasets import load_from_disk
from transformers import (
    set_seed,
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    TrainerCallback,
)


def load_config(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

class LoggingDataset:
    def __init__(self, dataset):
        self.dataset = dataset

    def __getitem__(self, idx):
        print(f"[ğŸ§¾] Using sample index: {idx}")
        return self.dataset[idx]

    def __len__(self):
        return len(self.dataset)

class SavePermanentCheckpointCallback(TrainerCallback):
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step % 200 == 0:
            src_dir = os.path.join(args.output_dir, f"checkpoint-{state.global_step}")
            dst_dir = os.path.join(args.output_dir, f"../save-point/checkpoint-{state.global_step}-permanent")
            if os.path.exists(src_dir) and not os.path.exists(dst_dir):
                shutil.copytree(src_dir, dst_dir)
                print(f"[âœ“] Permanent checkpoint saved at {dst_dir}")


def convert_to_gguf(gguf_name, output_dir):
    try:
        print("[â†’] GGUF ë³€í™˜ ì‹œì‘...")
        subprocess.run([
            "python", "../llama.cpp/convert.py",
            "--outfile", gguf_name,
            "--outdir", "./ouputGguf",
            output_dir
        ], check=True)
        print("[âœ“] GGUF ë³€í™˜ ì™„ë£Œ")
    except subprocess.CalledProcessError as e:
        print(f"[!] GGUF ë³€í™˜ ì‹¤íŒ¨: {e}")


def train_and_convert():
    print("[CPU ê¸°ë°˜ í•™ìŠµ ì‹œì‘]")

    config = load_config("config.json")
    base_model_path = config["base_model"]
    output_dir = config["output_dir"]
    gguf_name = config["gguf_name"]
    batch_size = config.get("batch_size", 1)
    epochs = config.get("epochs", 3)
    set_seed(77)

    print("[âœ“] ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float32
    ).to("cpu")  # ëª…ì‹œì  CPU ë¡œë”©

    print("[âœ“] í† í¬ë‚˜ì´ì¦ˆëœ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘: ./tokenized_dataset")
    dataset = LoggingDataset(load_from_disk("./tokenized_dataset").shuffle(seed=77))

    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        per_device_train_batch_size=batch_size,
        num_train_epochs=epochs,
        gradient_accumulation_steps=1,
        prediction_loss_only=True,
        logging_steps=1,
        save_strategy="steps",           # â† ì €ì¥ ì „ëµì„ "ìŠ¤í… ë‹¨ìœ„"ë¡œ ë³€ê²½
        save_steps=4,                    # â† 4 ìŠ¤í…ë§ˆë‹¤ ì €ì¥
        save_total_limit=2,             # â† ìµœê·¼ 2ê°œë§Œ ìœ ì§€ (ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½)
        fp16=False,                     # CPU ì „ìš©
        report_to="none",
        dataloader_num_workers=0        # CPU ë³‘ë ¬ ì²˜ë¦¬ ìµœì†Œí™”
    )

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        callbacks=[SavePermanentCheckpointCallback()]
    )

    print("[â†’] í•™ìŠµ ì‹œì‘...")
    output_dir = "outputs/distilled"
    checkpoint_exists = (
        os.path.isdir(output_dir)
        and any("checkpoint" in d for d in os.listdir(output_dir))
    )

    if checkpoint_exists:
        trainer.train(resume_from_checkpoint=True)
    else:
        trainer.train()
    print("[âœ“] í•™ìŠµ ì™„ë£Œ")

    print("[â†’] ëª¨ë¸ ì €ì¥ ì¤‘...")
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)

    convert_to_gguf(gguf_name, output_dir)


if __name__ == "__main__":
    train_and_convert()
